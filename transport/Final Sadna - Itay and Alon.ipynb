{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Open University Data Analysis Workshop Project\n",
    "\n",
    "Served by:\n",
    "Itay Cohen - 209146158\n",
    "Alon Gilda - 209146224\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "The goal of this project is to give us users a better way to plan their trips using public transportation.\n",
    " \n",
    "Often times when we plan a trip using public transportation and we count on the timings specified by the public transportation companies, we find ourselves waiting for the bus for a long time, or even worse, missing the bus and having to wait for the next one.\n",
    "\n",
    "This project aims to solve this problem by using real-time data to predict the arrival time of the bus to the station, and to give the user a better estimation of the arrival time of the bus to the station.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "We both live outside of Tel Aviv and have a long commute to work.\n",
    "\n",
    "We prefer to relay on public transportation rather than driving to work.\n",
    "\n",
    "Current applications and methods in Israel are far from accurate. We find ourselves not trusting the applications and prefer to use our own estimations based on our personal experience.\n",
    "\n",
    "We believe that we can use the data that is available to us to create a better estimation of the arrival time of the bus to the station.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de518e6535858269"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data sources - overview\n",
    "\n",
    "There is a lot of data around public transportation methods, most of it relies on government source.\n",
    "\n",
    "These sources listed below, are what we found best to describe the data we need to create our model. \n",
    "\n",
    "### GTFS\n",
    "\n",
    "GTFS is a standard format for public transportation data. It is used by many public transportation companies around the world.\n",
    "\n",
    "In Israel, GTFS data is being published in government websites, as long with some other private archives.\n",
    " \n",
    "You can find here a detailed example of what GTFS data looks like:\n",
    "\n",
    "[https://transitfeeds.com/p/ministry-of-transport-and-road-safety/820/latest/routes](https://transitfeeds.com/p/ministry-of-transport-and-road-safety/820/latest/routes)\n",
    "\n",
    "*GTFS Data only contains planned data, and does not contain real-time data.*\n",
    "\n",
    "### SIRI (Real time data)\n",
    "\n",
    "SIRI Data is the Israeli government name for it's real-time bus API.\n",
    "\n",
    "It's being published by a specific API upon request, and gives the opportunity to log real time data.\n",
    "\n",
    "[https://www.gov.il/he/departments/general/real_time_information_siri](https://www.gov.il/he/departments/general/real_time_information_siri)\n",
    "\n",
    "### Hasadna - Open Bus\n",
    "\n",
    "Hasadna is a non-profit organization, based on volunteers, that collects data from the Israeli government and publishes it to the public.\n",
    "\n",
    "\n",
    "Open Bus is a project by Hasadna that collects data from the SIRI API and publishes it to the public.\n",
    "\n",
    "They log the data into servers and allow querying it via an API called Stride.\n",
    "\n",
    "Hasadna Site : [www.hasadna.org.il](www.hasadna.org.il)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29a04fbec8a718c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Install dependencies\n",
    "\n",
    "This notebook requires two dependencies which can be installed with the following command `pip install pandas open-bus-stride-client`.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a565e4cc9dd4dec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import stride\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "from IPython.display import display\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "DATA_DIR = \"data\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:34.584665Z",
     "start_time": "2024-02-07T19:48:34.237051Z"
    }
   },
   "id": "1642f2dd399fffa4",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Collection\n",
    "\n",
    "Utilize the Stride API to collect data and explain the data we collected.\n",
    "\n",
    "The input we use is line numbers. I live close by to line 123, and Itay lives close by to line 202.\n",
    "\n",
    "The data displayed below is the first query in order to generate the official 'line_ref' and 'operator_ref', which are a more official version of the line number."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42b645b7d03cc77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO ADD MORE EXPLAINATIONS on this fields\n",
    "display(pd.DataFrame(stride.iterate('/gtfs_routes/list', {\"route_short_name\": \"123\", \"limit\": 3}, limit=3)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16cfb039ff7baa26",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Siri rides\n",
    "\n",
    "After getting the official line name, we collect data about the rides that we're done in that day.\n",
    "\n",
    "We'll show here a specific example for a operator ref + line ref\n",
    "\n",
    "Each bus ride has a special ID to it.\n",
    "\n",
    "The query needs to have a short time frame or else it will take a long time to run."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d7c76ffba385707"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO Add explanations on the output fields\n",
    "display(pd.DataFrame(stride.iterate('/siri_rides/list', {\n",
    "            \"gtfs_ride__start_time_from\": '2023-08-01T00:00:00+03:00',\n",
    "            \"scheduled_start_time_from\": '2023-08-01T00:00:00+03:00',\n",
    "            \"gtfs_ride__start_time_to\": '2023-08-02T00:00:00+03:00',\n",
    "            \"scheduled_start_time_to\": '2023-08-02T00:00:00+03:00',\n",
    "            \"gtfs_route__line_refs\": \"4135\",\n",
    "            \"limit\": 3\n",
    "        }, limit = 3)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efbe9208a1d436df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Siri ride stops\n",
    "\n",
    "We want to create predictions per station of the ride.\n",
    "\n",
    "We take each ride and query the stops that were made in that ride."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5df60dcdcd998690"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display(pd.DataFrame(stride.iterate('/siri_ride_stops/list', {\n",
    "            \"siri_ride_ids\": \"45194056\",\n",
    "            \"siri_ride__scheduled_start_time_from\": '2023-08-01T00:00:00+03:00',\n",
    "            \"siri_ride__scheduled_start_time_to\": '2023-08-02T00:00:00+03:00',\n",
    "            \"limit\": 3\n",
    "        }, limit=3)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c1c4c8a21d2adc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DataFetcher class - finalized data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b21d1435d0427fb5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "lines = [123, 202, 921, 970, 148, 55, 82, 17, 18, 138, 70]\n",
    "TIME_PERIOD_TRAIN = {\n",
    "    'start': '2022-06-01T00:00:00+03:00',\n",
    "    'end': '2022-07-01T00:00:00+03:00'\n",
    "}\n",
    "\n",
    "TIME_PERIOD_TEST = {\n",
    "    'start': '2023-06-01T00:00:00+03:00',\n",
    "    'end': '2023-07-01T00:00:00+03:00'\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:47:44.842754Z",
     "start_time": "2024-02-07T19:47:44.491794Z"
    }
   },
   "id": "cdf03f946cd64125",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataFetcher(object):\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 lines,\n",
    "                 start_date,\n",
    "                 end_date,\n",
    "                 time_period_per_sample_hop_in_days = 1,\n",
    "                 amount_of_lines_to_sample_per_hop = 5,\n",
    "                 limit_ratio_lines_to_gtfs_lines = 5,\n",
    "                 amount_of_rides_to_sample_per_hop = 100,\n",
    "                 limit_routes_per_single_day = 10000,\n",
    "                 limit_stops_per_single_day = 100000\n",
    "                 ):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.time_period_per_sample_hop_in_days = time_period_per_sample_hop_in_days\n",
    "        self.amount_of_lines_to_sample_per_hop = amount_of_lines_to_sample_per_hop\n",
    "        self.limit_ratio_lines_to_gtfs_lines = limit_ratio_lines_to_gtfs_lines\n",
    "        self.limit_routes_per_single_day = limit_routes_per_single_day\n",
    "        self.limit_stops_per_single_day = limit_stops_per_single_day\n",
    "        self.amount_of_rides_to_sample_per_hop = amount_of_rides_to_sample_per_hop\n",
    "        self.lines = lines\n",
    "        self.id = f\"{name}_{start_date}_{end_date}\"\n",
    "\n",
    "    def fetch(self):\n",
    "        # Check if data already exists\n",
    "        final_data_path = f\"{DATA_DIR}/{self.id}_final_data.csv\"\n",
    "        if os.path.exists(final_data_path):\n",
    "            return pd.read_csv(final_data_path)\n",
    "\n",
    "        # Fetch data\n",
    "        self.data = self.fetch_loop()\n",
    "        # Save data\n",
    "        self.cache_table(self.data, \"final_data\")\n",
    "        return self.data\n",
    "\n",
    "\n",
    "    def get_time_periods(self):\n",
    "        time_periods = []\n",
    "        # Time example - 2023-08-01T00:00:00+03:00\n",
    "        start_time_as_datetime = datetime.datetime.strptime(self.start_date, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        end_time_as_datetime = datetime.datetime.strptime(self.end_date, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "        current_time = start_time_as_datetime\n",
    "        while current_time < end_time_as_datetime:\n",
    "            current_time_as_str = current_time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "            next_time = current_time + timedelta(days=self.time_period_per_sample_hop_in_days)\n",
    "            next_time_as_str = next_time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "            time_periods.append((current_time_as_str, next_time_as_str))\n",
    "            current_time = next_time\n",
    "\n",
    "        return time_periods\n",
    "    def fetch_loop(self):\n",
    "        # One time convert lines to gtfs lines\n",
    "        self.gtfs_lines = self.convert_lines_to_gtfs_lines()\n",
    "        self.lines_refs_unq = self.gtfs_lines['line_ref'].unique()\n",
    "\n",
    "        full_siri_ride_stops = pd.DataFrame()\n",
    "        for start_time_as_str, end_time_as_str in self.get_time_periods():\n",
    "            print(f\"Fetching data for time period: {start_time_as_str} - {end_time_as_str}\")\n",
    "            siri_ride_stops_daily = self.fetch_single_time_period(start_time_as_str, end_time_as_str)\n",
    "            if siri_ride_stops_daily is None or siri_ride_stops_daily.empty:\n",
    "                continue\n",
    "            full_siri_ride_stops = pd.concat([full_siri_ride_stops, siri_ride_stops_daily])\n",
    "\n",
    "        return full_siri_ride_stops\n",
    "\n",
    "    def fetch_single_time_period(self, start_time_str, end_time_str):\n",
    "        # Randomly select lines refs to sample\n",
    "        sampled_line_refs = random.sample(list(self.lines_refs_unq), self.amount_of_lines_to_sample_per_hop)\n",
    "        print(f\"Sampled line refs: {sampled_line_refs}\")\n",
    "        # Get rides for those lines\n",
    "        siri_rides = self.read_siri_rides(sampled_line_refs, start_time_str, end_time_str)\n",
    "        # return if siri rides is empty\n",
    "        if siri_rides.empty:\n",
    "            return pd.DataFrame()\n",
    "        siri_ride_ids_unq = siri_rides['id'].unique()\n",
    "        # Get stops for those rides\n",
    "        siri_ride_stops = self.read_siri_ride_stops(siri_ride_ids_unq, start_time_str, end_time_str)\n",
    "        return siri_ride_stops\n",
    "\n",
    "    def convert_lines_to_gtfs_lines(self):\n",
    "        cached_data = self.read_cached_table(\"lines_data\")\n",
    "        if cached_data is not None:\n",
    "            return cached_data\n",
    "\n",
    "        lines_data = pd.DataFrame()\n",
    "\n",
    "        # Iterate each line and append to the lines_data dataframe\n",
    "        for line in self.lines:\n",
    "            new_line_by_short_name = pd.DataFrame(\n",
    "                stride.iterate('/gtfs_routes/list', {\"route_short_name\": line, \"limit\": 5}, limit=5))\n",
    "\n",
    "            lines_data = pd.concat([lines_data, new_line_by_short_name])\n",
    "\n",
    "        self.cache_table(lines_data, \"lines_data\")\n",
    "        return lines_data\n",
    "\n",
    "    def read_siri_rides(self, line_refs_unq, start_time_str, end_time_str):\n",
    "        table_name = f\"siri_rides_{start_time_str}_{end_time_str}\"\n",
    "        cached_data = self.read_cached_table(table_name)\n",
    "        if cached_data is not None:\n",
    "            return cached_data\n",
    "\n",
    "        # Get GTFS Data\n",
    "        siri_rides = pd.DataFrame(stride.iterate('/siri_rides/list', {\n",
    "            \"gtfs_ride__start_time_from\": start_time_str,\n",
    "            \"scheduled_start_time_from\": start_time_str,\n",
    "            \"gtfs_ride__start_time_to\": end_time_str,\n",
    "            \"scheduled_start_time_to\": end_time_str,\n",
    "            \"gtfs_route__line_refs\": \",\".join([str(line_ref) for line_ref in line_refs_unq]),\n",
    "            \"limit\": self.limit_routes_per_single_day\n",
    "        }, limit = self.limit_routes_per_single_day))\n",
    "\n",
    "        self.cache_table(siri_rides, table_name)\n",
    "        return siri_rides\n",
    "\n",
    "    def read_siri_ride_stops(self, siri_ride_ids_unq, start_time_str, end_time_str):\n",
    "        table_name = f\"siri_ride_stops_{start_time_str}_{end_time_str}\"\n",
    "        cached_data = self.read_cached_table(table_name)\n",
    "        if cached_data is not None:\n",
    "            return cached_data\n",
    "\n",
    "        # Randomly sample rids ids\n",
    "        if len(siri_ride_ids_unq) > self.amount_of_rides_to_sample_per_hop:\n",
    "            print(f\"Sampling {self.amount_of_rides_to_sample_per_hop} rides out of {len(siri_ride_ids_unq)}\")\n",
    "            siri_ride_ids_unq = random.sample(list(siri_ride_ids_unq), self.amount_of_rides_to_sample_per_hop)\n",
    "\n",
    "        # Get GTFS Data\n",
    "        siri_ride_stops = pd.DataFrame(stride.iterate('/siri_ride_stops/list', {\n",
    "            \"siri_ride_ids\": \",\".join([str(siri_ride_id) for siri_ride_id in siri_ride_ids_unq]),\n",
    "            \"siri_ride__scheduled_start_time_from\": start_time_str,\n",
    "            \"siri_ride__scheduled_start_time_to\": end_time_str,\n",
    "            \"limit\": self.limit_stops_per_single_day\n",
    "        }, limit=self.limit_stops_per_single_day))\n",
    "\n",
    "        self.cache_table(siri_ride_stops, table_name)\n",
    "        return siri_ride_stops\n",
    "\n",
    "    def cache_table(self, table, table_name):\n",
    "        table.to_csv(f\"{DATA_DIR}/{self.id}_{table_name}.csv\")\n",
    "\n",
    "    def read_cached_table(self, table_name):\n",
    "        path = f\"{DATA_DIR}/{self.id}_{table_name}.csv\"\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "        table = pd.read_csv(path)\n",
    "        # remove unnamed column\n",
    "        table = table.loc[:, ~table.columns.str.contains('^Unnamed')]\n",
    "        return table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:41.659742Z",
     "start_time": "2024-02-07T19:48:41.601315Z"
    }
   },
   "id": "229c8ccd6aac71ca",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_test = DataFetcher(\n",
    "    name = \"single_month_prod_test_set\",\n",
    "    lines = lines,\n",
    "    start_date = TIME_PERIOD_TEST['start'],\n",
    "    end_date = TIME_PERIOD_TEST['end'],\n",
    "    time_period_per_sample_hop_in_days = 1,\n",
    "    amount_of_lines_to_sample_per_hop = 10,\n",
    "    limit_ratio_lines_to_gtfs_lines = 5,\n",
    "    amount_of_rides_to_sample_per_hop = 100,\n",
    "    limit_routes_per_single_day = 1000,\n",
    "    limit_stops_per_single_day = 10000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:43.520149Z",
     "start_time": "2024-02-07T19:48:43.173497Z"
    }
   },
   "id": "3d8f97590a3f50f4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_data = data_test.fetch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:49.712943Z",
     "start_time": "2024-02-07T19:48:43.945265Z"
    }
   },
   "id": "753e390bff8ee98",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = DataFetcher(\n",
    "    name = \"single_month_prod_train_set_half_day\",\n",
    "    lines = lines,\n",
    "    start_date = TIME_PERIOD_TRAIN['start'],\n",
    "    end_date = TIME_PERIOD_TRAIN['end'],\n",
    "    time_period_per_sample_hop_in_days = 0.5,\n",
    "    amount_of_lines_to_sample_per_hop = 10,\n",
    "    limit_ratio_lines_to_gtfs_lines = 5,\n",
    "    amount_of_rides_to_sample_per_hop = 100,\n",
    "    limit_routes_per_single_day = 1000,\n",
    "    limit_stops_per_single_day = 10000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:49.790967Z",
     "start_time": "2024-02-07T19:48:49.584690Z"
    }
   },
   "id": "4f176f20bf867dfc",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data = train_data.fetch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:56.370178Z",
     "start_time": "2024-02-07T19:48:49.600461Z"
    }
   },
   "id": "edd75dc95e821914",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA - Exploratory Data Analysis\n",
    "\n",
    "In this section we will work with our main training set to get a good understanding of the data.\n",
    "\n",
    "The purpose of this section is to:\n",
    " \n",
    "1. Get a better understanding of the data we are working with\n",
    "2. Explore ideas for features\n",
    "3. Data cleaning methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a95a783b2804dd4d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = training_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee3bbdc26666873",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values in each column\n",
    "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "\n",
    "# Displaying columns with missing values and their corresponding percentages\n",
    "missing_data_summary = pd.DataFrame({'Column': missing_percentage.index, 'MissingPercentage': missing_percentage.values})\n",
    "missing_data_summary = missing_data_summary[missing_data_summary.MissingPercentage > 0]\n",
    "missing_data_summary.sort_values(by='MissingPercentage', ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e67196ee6a858fc3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our target variable is the difference between the actual arrival time and the scheduled arrival time.\n",
    "\n",
    "This is why we need to filter our nulls for the nearest_siri_vehicle_location__siri_ride_stop_id and gtfs_ride_stop__arrival_time.\n",
    "\n",
    "This is obviously an estimate since the exact arrival time is based on the SIRI measurements, which are being queried every other minute, approximately, according the Hasadna. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "687fdc625c58bb7b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = data[data.nearest_siri_vehicle_location__siri_ride_stop_id.notnull() & data.gtfs_ride_stop__arrival_time.notnull()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afc2167470921f0f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values in each column\n",
    "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "\n",
    "# Displaying columns with missing values and their corresponding percentages\n",
    "missing_data_summary = pd.DataFrame({'Column': missing_percentage.index, 'MissingPercentage': missing_percentage.values})\n",
    "missing_data_summary = missing_data_summary[missing_data_summary.MissingPercentage > 0]\n",
    "missing_data_summary.sort_values(by='MissingPercentage', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6730c0c1c296daf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! a lot less nulls.\n",
    "Let's remove the siri_ride__journey_gtfs_ride_id column from our data, as it is not relevant and has a high null percentage.\n",
    "The column "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6da6a27acb8251b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = data.drop(columns=['siri_ride__journey_gtfs_ride_id'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d3038bbf55063da",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Correlation heatmap\n",
    "\n",
    "In the map below we can see a lot of columns having 1 correlativty, specificaly columns that related to id.\n",
    "\n",
    "Let's remove all of them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12b74983788ff7b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Correlations between columns\n",
    "corr = data.corr()\n",
    "\n",
    "# Plot a big heatmap of the correlations\n",
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "sns.heatmap(corr, annot=True, cmap=\"RdYlGn\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4baea43832d3983e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculating correlation matrix\n",
    "corr_matrix = data.corr().abs()\n",
    "\n",
    "# Initializing a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adding edges between highly correlated columns\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] >= 0.995:  # Threshold for adding an edge\n",
    "            G.add_edge(corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "\n",
    "# Finding connected components (groups of correlated columns)\n",
    "connected_components = list(nx.connected_components(G))\n",
    "\n",
    "# Initialize mappings of columns to remove and to keep\n",
    "columns_to_keep = set()\n",
    "replacement_mapping = {}\n",
    "\n",
    "for group in connected_components:\n",
    "    # Selecting the first column in each group to keep\n",
    "    keep = next(iter(group))\n",
    "    columns_to_keep.add(keep)\n",
    "    # Mapping the rest for removal\n",
    "    for col in group:\n",
    "        if col != keep:\n",
    "            replacement_mapping[col] = keep\n",
    "\n",
    "# Removing columns that are not kept\n",
    "columns_to_remove = list(set(replacement_mapping.keys()))\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Print statements for replaced columns\n",
    "for col, replacement in replacement_mapping.items():\n",
    "    print(f\"Removed column: {col}, replaced with: {replacement}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e77e2e19245fb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "\n",
    "# Plot a big heatmap of the correlations\n",
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "sns.heatmap(corr, annot=True, cmap=\"RdYlGn\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4beacb890a58d87",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO - Add more EDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f87fbc4b92d560a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FeatureCreator - finalized feature creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df925d621e652704"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# jewish holidays in feauture\n",
    "JEWISH_HOLIDAYS = {\n",
    "    \"Pesach23\": {\"from\": \"2023-04-05\", \"to\": \"2023-04-12\"},\n",
    "    \"Shavuot23\": {\"from\": \"2023-05-25\", \"to\": \"2023-05-26\"},\n",
    "    \"Rosh Hashanah23\": {\"from\": \"2023-09-15\", \"to\": \"2023-09-17\"},\n",
    "    \"Yom Kippur23\": {\"from\": \"2023-09-24\", \"to\": \"2023-09-25\"},\n",
    "    \"Sukkot23\": {\"from\": \"2023-09-30\", \"to\": \"2023-10-05\"},\n",
    "    \"Hanukkah23\": {\"from\": \"2023-12-07\", \"to\": \"2023-12-15\"},\n",
    "    \"Pesach22\": {\"from\": \"2022-04-15\", \"to\": \"2022-04-22\"},\n",
    "    \"Shavuot22\": {\"from\": \"2022-06-04\", \"to\": \"2022-06-05\"},\n",
    "    \"Rosh Hashanah22\": {\"from\": \"2022-09-25\", \"to\": \"2022-09-27\"},\n",
    "    \"Yom Kippur22\": {\"from\": \"2022-10-04\", \"to\": \"2022-10-05\"},\n",
    "    \"Sukkot22\": {\"from\": \"2022-10-09\", \"to\": \"2022-10-16\"},\n",
    "    \"Hanukkah22\": {\"from\": \"2022-12-18\", \"to\": \"2022-12-26\"},\n",
    "}\n",
    "EXTENDED_HOLIDAYS = {}\n",
    "for holiday, date_range in JEWISH_HOLIDAYS.items():\n",
    "    from_date = datetime.datetime.strptime(date_range['from'], '%Y-%m-%d')\n",
    "    to_date = datetime.datetime.strptime(date_range['to'], '%Y-%m-%d')\n",
    "\n",
    "    extended_from_date = from_date - timedelta(days=3)\n",
    "    extended_to_date = to_date + timedelta(days=3)\n",
    "\n",
    "    EXTENDED_HOLIDAYS[holiday] = {\"from\": extended_from_date, \"to\": extended_to_date}\n",
    "\n",
    "\n",
    "class FeatureCreator(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 train_or_test,\n",
    "                 siri_time_original_column_name = \"nearest_siri_vehicle_location__recorded_at_time\",\n",
    "                 gtfs_time_original_column_name = \"gtfs_ride_stop__arrival_time\",\n",
    "                 target_column_name = \"scheduled_vs_real_time_difference_seconds\"\n",
    "                 ):\n",
    "        if train_or_test not in [\"train\", \"test\"]:\n",
    "            raise Exception(\"train_or_test must be either train or test\")\n",
    "        self.train_or_test = train_or_test\n",
    "        self.data = data\n",
    "        self.siri_time_original_column_name = siri_time_original_column_name\n",
    "        self.gtfs_time_original_column_name = gtfs_time_original_column_name\n",
    "        self.target_column_name = target_column_name\n",
    "        self.remove_target_nulls()\n",
    "        self.clean_data()\n",
    "        self.create_target_variable_if_not_exists()\n",
    "    \n",
    "    def remove_target_nulls(self):\n",
    "        self.data = self.data[self.data[self.siri_time_original_column_name].notnull() & self.data[self.gtfs_time_original_column_name].notnull()]\n",
    "        \n",
    "    def clean_data(self):\n",
    "        # Filter outliers in the data\n",
    "        self.data = self.data[self.data['nearest_siri_vehicle_location__distance_from_siri_ride_stop_meters'] < 1000]\n",
    "        self.data = self.data[self.data['scheduled_vs_real_time_difference_seconds'] < 3600]\n",
    "        self.data = self.data[self.data['scheduled_vs_real_time_difference_seconds'] > -1200]\n",
    "    \n",
    "    def create_target_variable_if_not_exists(self):\n",
    "        if self.target_column_name not in self.data.columns:\n",
    "            self.data[self.siri_time_original_column_name] = pd.to_datetime(self.data[self.siri_time_original_column_name]).dt.tz_localize(None)\n",
    "            self.data[self.gtfs_time_original_column_name] = pd.to_datetime(self.data[self.gtfs_time_original_column_name]).dt.tz_localize(None)\n",
    "            self.data[\"scheduled_vs_real_time_difference\"] = self.data[self.siri_time_original_column_name] - self.data[self.gtfs_time_original_column_name]\n",
    "            self.data[self.target_column_name] = self.data[\"scheduled_vs_real_time_difference\"].dt.total_seconds()\n",
    "\n",
    "    def create_features(self):\n",
    "        self.create_is_holiday_feature()\n",
    "        self.create_population_features()\n",
    "        self.create_weather_features()\n",
    "        self.create_time_features()\n",
    "\n",
    "        self.create_sequence_features()\n",
    "        if self.train_or_test == \"train\":\n",
    "            self.create_target_encoded_variable(\"gtfs_stop__city\")\n",
    "            self.create_target_encoded_variable(\"gtfs_route__operator_ref\")\n",
    "            # self.create_lagging_feature()\n",
    "        else:\n",
    "            self.create_target_encoded_variable_from_dict(\"gtfs_stop__city\")\n",
    "            self.create_target_encoded_variable_from_dict(\"gtfs_route__operator_ref\")\n",
    "            # self.create_lagging_feature()\n",
    "        self.data[\"amount_cities_in_route\"] = self.data.groupby(\"siri_ride_id\")[\"gtfs_stop__city\"].transform(\"nunique\")\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_weather_features(self):\n",
    "        # read from rain.csv\n",
    "        rain_df = pd.read_csv('one_file_rain.csv')\n",
    "\n",
    "        rain_df['date_to_join'] = pd.to_datetime(rain_df['תאריך'], format=\"%d/%m/%Y\")\n",
    "        rain_per_day = rain_df.groupby(\"date_to_join\").count().reset_index()[[\"date_to_join\", \"תאריך\"]].rename(\n",
    "            columns={\"תאריך\": \"rain_exists\"})\n",
    "        self.data[\"date_to_join\"] = self.data[\"nearest_siri_vehicle_location__recorded_at_time\"].dt.date\n",
    "        self.data[\"date_to_join\"] = pd.to_datetime(self.data[\"date_to_join\"])\n",
    "        self.data = pd.merge(self.data, rain_per_day, on=\"date_to_join\", how=\"left\")\n",
    "        self.data[\"rain_exists\"] = self.data[\"rain_exists\"].apply(lambda x: True if x > 0 else False)\n",
    "\n",
    "    def create_is_holiday_feature(self):\n",
    "        def is_ride_in_holiday(ride, extended_holidays):\n",
    "            for holiday in extended_holidays.values():\n",
    "                if ride['nearest_siri_vehicle_location__recorded_at_time'] >= holiday['from'] and ride['nearest_siri_vehicle_location__recorded_at_time'] <= holiday['to']:\n",
    "                    return True\n",
    "\n",
    "            return False\n",
    "\n",
    "        self.data['is_holiday'] = self.data.apply(lambda ride: is_ride_in_holiday(ride, EXTENDED_HOLIDAYS), axis=1)\n",
    "        return True\n",
    "\n",
    "    def create_population_features(self):\n",
    "\n",
    "        # Assuming df is your DataFrame\n",
    "        cities = self.data['gtfs_stop__city'].unique()\n",
    "\n",
    "        # Initialize cities_dict with 0 as values for all cities\n",
    "        cities_dict = {re.sub(r'[^א-ת]', '', city): 0 for city in cities}\n",
    "\n",
    "        # Read data for city from population.csv\n",
    "        population_df = pd.read_csv('population.csv')\n",
    "\n",
    "        # Extract relevant columns from population_df\n",
    "        city_name_column = population_df.columns[1]\n",
    "        population_column = population_df.columns[7]\n",
    "\n",
    "        # Keep only hebrew alphabet characters in the city name\n",
    "        population_df[city_name_column] = population_df[city_name_column].apply(lambda x: re.sub(r'[^א-ת]', '', str(x)))\n",
    "\n",
    "        # Run over all the cities in the df and add the score to the dict\n",
    "        for city_orig in cities:\n",
    "            city = re.sub(r'[^א-ת]', '', city_orig)\n",
    "            try:\n",
    "                city_population_str = \\\n",
    "                population_df.loc[population_df[city_name_column] == city, population_column].values[0]\n",
    "                print(city_population_str + \" \" + city)\n",
    "                # Convert the population from string to integer\n",
    "                city_population = int(\n",
    "                    city_population_str.replace(',', ''))  # assuming population values might have commas\n",
    "                cities_dict[city] = city_population  # math.floor(city_population / 100000)\n",
    "            except (ValueError, IndexError):\n",
    "                # Handle cases where the population value is not a valid integer or city is not found\n",
    "                print(f\"Error processing city: {city}\")\n",
    "\n",
    "        # add the value of the city to the df\n",
    "        self.data['city_population'] = self.data['gtfs_stop__city'].apply(lambda x: cities_dict[re.sub(r'[^א-ת]', '', str(x))])\n",
    "\n",
    "    def create_time_features(self):\n",
    "        self.data['weekday'] = self.data['nearest_siri_vehicle_location__recorded_at_time'].apply(\n",
    "            lambda x: pd.to_datetime(x).weekday())\n",
    "        self.data[\"hour_of_day\"] = self.data[\"nearest_siri_vehicle_location__recorded_at_time\"].dt.hour\n",
    "\n",
    "\n",
    "    def create_target_encoded_variable_from_dict(self, column_name):\n",
    "        # read csv file with the dictionary mapping of the target encoded variable\n",
    "        target_encoded_dict = pd.read_csv(\"data/\" + column_name + \"_target_encoded.csv\")\n",
    "        # Merge the dictionary mapping of the target encoded variable with the data\n",
    "        self.data = pd.merge(self.data, target_encoded_dict, on=column_name, how=\"left\")\n",
    "        # Fill nulls with 0\n",
    "        self.data[column_name + \"_target_encoded\"] = self.data[column_name + \"_target_encoded\"].fillna(0)\n",
    "\n",
    "    def create_target_encoded_variable(self, column_name):\n",
    "        self.data[column_name + \"_target_encoded\"] = self.data.groupby(column_name)[self.target_column_name].transform('mean')\n",
    "        # Fill nulls with 0\n",
    "        self.data[column_name + \"_target_encoded\"] = self.data[column_name + \"_target_encoded\"].fillna(0)\n",
    "        # Keep the dictionary mapping of the target encoded variable in an external csv file\n",
    "        self.data[[column_name, column_name + \"_target_encoded\"]].drop_duplicates().to_csv(\"data/\" + column_name + \"_target_encoded.csv\", index=False)\n",
    "\n",
    "    def create_sequence_features(self):\n",
    "        max_stop_sequences = self.data.groupby(\"gtfs_ride__gtfs_route_id\").agg(\n",
    "            {\"gtfs_ride_stop__stop_sequence\": \"max\"}).reset_index().rename(\n",
    "                columns={\"gtfs_ride_stop__stop_sequence\": \"max_stop_sequence\"}\n",
    "        )\n",
    "        self.data = pd.merge(self.data, max_stop_sequences, on=\"gtfs_ride__gtfs_route_id\")\n",
    "        self.data[\"stop_sequence_ratio\"] = (self.data[\"gtfs_ride_stop__stop_sequence\"] / self.data[\"max_stop_sequence\"]).round(2)\n",
    "        self.data[\"stop_sequence\"] = self.data[\"gtfs_ride_stop__stop_sequence\"]\n",
    "\n",
    "    def create_lagging_feature(self):\n",
    "        # Make a window function taking the mean of the target variable for the previous 5 stops\n",
    "        self.data = self.data.sort_values(by=['siri_ride_id', 'stop_sequence'])\n",
    "        self.data[\"past_3_mean\"] = self.data.groupby('siri_ride_id')['scheduled_vs_real_time_difference_seconds']\\\n",
    "            .rolling(window=3,\n",
    "                     min_periods=1,\n",
    "                     closed='left')\\\n",
    "            .mean()\\\n",
    "            .reset_index(level=0, drop=True)\n",
    "\n",
    "        self.data[\"past_3_mean\"] = self.data[\"past_3_mean\"].fillna(0)\n",
    "\n",
    "\n",
    "# training_data = pd.read_csv(\"data/dev_test_train_2022-05-01T00:00:00+03:00_2022-10-01T00:00:00+03:00_final_data.csv\")\n",
    "#\n",
    "# training_data[\"nearest_siri_vehicle_location__recorded_at_time\"] = pd.to_datetime(training_data[\"nearest_siri_vehicle_location__recorded_at_time\"]).dt.tz_localize(None)\n",
    "# training_data[\"gtfs_ride_stop__arrival_time\"] = pd.to_datetime(training_data[\"gtfs_ride_stop__arrival_time\"]).dt.tz_localize(None)\n",
    "# training_data[\"scheduled_vs_real_time_difference\"] = training_data[\"nearest_siri_vehicle_location__recorded_at_time\"] - training_data[\"gtfs_ride_stop__arrival_time\"]\n",
    "# training_data[\"scheduled_vs_real_time_difference_seconds\"] = training_data[\"scheduled_vs_real_time_difference\"].dt.total_seconds()\n",
    "#\n",
    "#\n",
    "# print(FeatureCreator(training_data).create_features())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:09:15.626477Z",
     "start_time": "2024-02-07T20:09:15.161425Z"
    }
   },
   "id": "876463de4d68b530",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final sets creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d10b54bce78a1ffc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = ['is_holiday', 'city_population', 'rain_exists', 'weekday', 'hour_of_day', 'stop_sequence_ratio',\n",
    "    'gtfs_stop__city_target_encoded', 'gtfs_route__operator_ref_target_encoded', 'amount_cities_in_route']\n",
    "\n",
    "TARGET_COLUMN = 'scheduled_vs_real_time_difference_seconds'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:48:58.412371Z",
     "start_time": "2024-02-07T19:48:58.223152Z"
    }
   },
   "id": "ebc5ba2a6a53ccc0",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.siri_time_original_column_name] = pd.to_datetime(self.data[self.siri_time_original_column_name]).dt.tz_localize(None)\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.gtfs_time_original_column_name] = pd.to_datetime(self.data[self.gtfs_time_original_column_name]).dt.tz_localize(None)\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[\"scheduled_vs_real_time_difference\"] = self.data[self.siri_time_original_column_name] - self.data[self.gtfs_time_original_column_name]\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.target_column_name] = self.data[\"scheduled_vs_real_time_difference\"].dt.total_seconds()\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['is_holiday'] = self.data.apply(lambda ride: is_ride_in_holiday(ride, EXTENDED_HOLIDAYS), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474,530 תלאביביפו\n",
      "154,694 ביתשמש\n",
      "172,486 רמתגן\n",
      "218,357 בניברק\n",
      "255,387 פתחתקווה\n",
      "28,994 גבעתשמואל\n",
      "Error processing city: מטהיהודה\n",
      "951 מטע\n",
      "64,016 ביתרעילית\n",
      "Error processing city: קריתאונו\n",
      "290,306 חיפה\n",
      "73,678 ראשהעין\n",
      "23,761 נשר\n",
      "24,574 גןיבנה\n",
      "5,997 קיסריה\n",
      "44,840 פרדסחנהכרכור\n",
      "Error processing city: דרוםהשרון\n",
      "21,097 גבעתזאב\n",
      "16,281 בנימינהגבעתעדה\n",
      "981,711 ירושלים\n",
      "24,145 זכרוןיעקב\n",
      "226,827 אשדוד\n",
      "13,722 פוריידיס\n",
      "Error processing city: חוףהכרמל\n",
      "233,104 נתניה\n",
      "28,753 טירתכרמל\n",
      "Error processing city: מטהבנימין\n",
      "36,776 דימונה\n",
      "Error processing city: רמתהנגב\n",
      "11,170 ירוחם\n",
      "516 מרחבעם\n",
      "470 שדהבוקר\n",
      "5,263 מצפהרמון\n",
      "78,007 נצרת\n",
      "44,184 נוףהגליל\n",
      "31,405 יהודמונוסון\n",
      "38,854 אוריהודה\n",
      "1,360 חמד\n",
      "7,823 ביתדגן\n",
      "260,453 ראשוןלציון\n",
      "12,142 צורהדסה\n",
      "103,041 חדרה\n",
      "Error processing city: מנשה\n",
      "Error processing city: הרצליה\n",
      "80,260 רעננה\n",
      "101,556 כפרסבא\n",
      "1,938 מדרשתבןגוריון\n",
      "Error processing city: עמקחפר\n",
      "715 אלישיב\n",
      "29,450 כפריונה\n",
      "Error processing city: פרדסיה\n",
      "23,220 קדימהצורן\n",
      "Error processing city: לבהשרון\n",
      "Error processing city: חבלמודיעין\n",
      "14,477 תלמונד\n",
      "22,521 מעלותתרשיחא\n",
      "99,171 מודיעיןמכביםרעות\n",
      "65,614 הודהשרון\n",
      "5,562 כפרורדים\n",
      "1,178 בצרה\n",
      "128,465 בתים\n",
      "153,138 אשקלון\n",
      "Error processing city: מרוםהגליל\n",
      "23,998 מגאר\n",
      "492 טפחות\n",
      "293 חזון\n",
      "781 כפרחנניה\n",
      "920 עיןאלאסד\n",
      "1,136 מירון\n",
      "1,092 בריוחאי\n",
      "428 שפר\n",
      "260 כלנית\n",
      "38,029 צפת\n",
      "61,924 גבעתיים\n",
      "23,646 גניתקווה\n",
      "4,054 סביון\n",
      "38,046 מעלהאדומים\n",
      "19,397 ריינה\n",
      "Error processing city: חוףהשרון\n",
      "14,365 אבןיהודה\n",
      "20,874 אורעקיבא\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['city_population'] = self.data['gtfs_stop__city'].apply(lambda x: cities_dict[re.sub(r'[^א-ת]', '', str(x))])\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[\"date_to_join\"] = self.data[\"nearest_siri_vehicle_location__recorded_at_time\"].dt.date\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[\"date_to_join\"] = pd.to_datetime(self.data[\"date_to_join\"])\n"
     ]
    }
   ],
   "source": [
    "training_set = FeatureCreator(training_data, \"train\").create_features()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:49:40.522683Z",
     "start_time": "2024-02-07T19:48:59.107107Z"
    }
   },
   "id": "c8c7ce2fc2ec6940",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.siri_time_original_column_name] = pd.to_datetime(self.data[self.siri_time_original_column_name]).dt.tz_localize(None)\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.gtfs_time_original_column_name] = pd.to_datetime(self.data[self.gtfs_time_original_column_name]).dt.tz_localize(None)\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[\"scheduled_vs_real_time_difference\"] = self.data[self.siri_time_original_column_name] - self.data[self.gtfs_time_original_column_name]\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.target_column_name] = self.data[\"scheduled_vs_real_time_difference\"].dt.total_seconds()\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['is_holiday'] = self.data.apply(lambda ride: is_ride_in_holiday(ride, EXTENDED_HOLIDAYS), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73,678 ראשהעין\n",
      "Error processing city: דרוםהשרון\n",
      "255,387 פתחתקווה\n",
      "290,306 חיפה\n",
      "99,171 מודיעיןמכביםרעות\n",
      "Error processing city: חבלמודיעין\n",
      "38,854 אוריהודה\n",
      "172,486 רמתגן\n",
      "218,357 בניברק\n",
      "31,405 יהודמונוסון\n",
      "1,360 חמד\n",
      "7,823 ביתדגן\n",
      "260,453 ראשוןלציון\n",
      "28,753 טירתכרמל\n",
      "Error processing city: חוףהכרמל\n",
      "13,722 פוריידיס\n",
      "24,145 זכרוןיעקב\n",
      "16,281 בנימינהגבעתעדה\n",
      "44,840 פרדסחנהכרכור\n",
      "5,997 קיסריה\n",
      "23,761 נשר\n",
      "19,397 ריינה\n",
      "78,007 נצרת\n",
      "103,041 חדרה\n",
      "Error processing city: מנשה\n",
      "981,711 ירושלים\n",
      "38,046 מעלהאדומים\n",
      "128,465 בתים\n",
      "474,530 תלאביביפו\n",
      "5,263 מצפהרמון\n",
      "Error processing city: רמתהנגב\n",
      "1,938 מדרשתבןגוריון\n",
      "470 שדהבוקר\n",
      "516 מרחבעם\n",
      "11,170 ירוחם\n",
      "36,776 דימונה\n",
      "233,104 נתניה\n",
      "154,694 ביתשמש\n",
      "Error processing city: מטהיהודה\n",
      "951 מטע\n",
      "12,142 צורהדסה\n",
      "64,016 ביתרעילית\n",
      "38,029 צפת\n",
      "21,097 גבעתזאב\n",
      "Error processing city: מטהבנימין\n",
      "Error processing city: הרצליה\n",
      "80,260 רעננה\n",
      "101,556 כפרסבא\n",
      "22,521 מעלותתרשיחא\n",
      "226,827 אשדוד\n",
      "5,562 כפרורדים\n",
      "24,574 גןיבנה\n",
      "28,994 גבעתשמואל\n",
      "153,138 אשקלון\n",
      "Error processing city: מרוםהגליל\n",
      "23,998 מגאר\n",
      "492 טפחות\n",
      "260 כלנית\n",
      "Error processing city: קריתאונו\n",
      "65,614 הודהשרון\n",
      "61,924 גבעתיים\n",
      "1,178 בצרה\n",
      "Error processing city: חוףהשרון\n",
      "14,365 אבןיהודה\n",
      "Error processing city: לבהשרון\n",
      "29,450 כפריונה\n",
      "23,646 גניתקווה\n",
      "Error processing city: עמקחפר\n",
      "20,874 אורעקיבא\n",
      "715 אלישיב\n",
      "Error processing city: פרדסיה\n",
      "23,220 קדימהצורן\n",
      "14,477 תלמונד\n",
      "293 חזון\n",
      "781 כפרחנניה\n",
      "920 עיןאלאסד\n",
      "1,136 מירון\n",
      "1,092 בריוחאי\n",
      "44,184 נוףהגליל\n",
      "428 שפר\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['city_population'] = self.data['gtfs_stop__city'].apply(lambda x: cities_dict[re.sub(r'[^א-ת]', '', str(x))])\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[\"date_to_join\"] = self.data[\"nearest_siri_vehicle_location__recorded_at_time\"].dt.date\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2565796405.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[\"date_to_join\"] = pd.to_datetime(self.data[\"date_to_join\"])\n"
     ]
    }
   ],
   "source": [
    "test_set = FeatureCreator(test_data, \"test\").create_features()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:50:08.954676Z",
     "start_time": "2024-02-07T19:49:40.565263Z"
    }
   },
   "id": "237396fb8f375b6f",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models trainings\n",
    "\n",
    "In this section we will train our models and evaluate them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a9f5c9969370dab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install statsmodels\n",
    "# ! pip install xgboost\n",
    "# ! pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:50:09.092446Z",
     "start_time": "2024-02-07T19:50:08.956309Z"
    }
   },
   "id": "2a61b6264b034f09",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = training_set[FEATURE_COLUMNS]\n",
    "y_train = training_set[TARGET_COLUMN]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:50:09.219694Z",
     "start_time": "2024-02-07T19:50:08.968921Z"
    }
   },
   "id": "b9662696fbdcb920",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:50:09.224729Z",
     "start_time": "2024-02-07T19:50:09.113079Z"
    }
   },
   "id": "618a007ca738d59a",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2306361139.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['is_holiday'] = X_train['is_holiday'].astype(int)\n",
      "/var/folders/0b/t8rt5b356vx0nj96957ylbxh0000gn/T/ipykernel_69361/2306361139.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['rain_exists'] = X_train['rain_exists'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Convert bool columns to numberis\n",
    "X_train['is_holiday'] = X_train['is_holiday'].astype(int)\n",
    "X_train['rain_exists'] = X_train['rain_exists'].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:01:07.721331Z",
     "start_time": "2024-02-07T20:01:07.200330Z"
    }
   },
   "id": "5cae726e79d6f3c",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2254/2254 [==============================] - 39s 15ms/step - loss: 567225.7500 - val_loss: 445146.8125\n",
      "Epoch 2/10\n",
      "2254/2254 [==============================] - 33s 15ms/step - loss: 502632.1562 - val_loss: 402872.3125\n",
      "Epoch 3/10\n",
      "2254/2254 [==============================] - 28s 12ms/step - loss: 459676.0000 - val_loss: 374843.0000\n",
      "Epoch 4/10\n",
      "2254/2254 [==============================] - 33s 14ms/step - loss: 429624.4688 - val_loss: 355026.8125\n",
      "Epoch 5/10\n",
      "2254/2254 [==============================] - 31s 14ms/step - loss: 403881.1875 - val_loss: 334970.9375\n",
      "Epoch 6/10\n",
      "2254/2254 [==============================] - 29s 13ms/step - loss: 377664.3750 - val_loss: 313468.1250\n",
      "Epoch 7/10\n",
      "2254/2254 [==============================] - 28s 12ms/step - loss: 354916.5938 - val_loss: 298350.8125\n",
      "Epoch 8/10\n",
      "2254/2254 [==============================] - 29s 13ms/step - loss: 338648.4688 - val_loss: 290458.1875\n",
      "Epoch 9/10\n",
      "2254/2254 [==============================] - 35s 15ms/step - loss: 324593.5000 - val_loss: 293674.5000\n",
      "Epoch 10/10\n",
      "2254/2254 [==============================] - 36s 16ms/step - loss: 316079.8750 - val_loss: 282826.0000\n"
     ]
    }
   ],
   "source": [
    "X_train_ndarray = X_train.to_numpy()\n",
    "# Assuming X_train and y_train are your training data and labels, respectively\n",
    "# Reshape input data for LSTM [samples, time steps, features]\n",
    "X_train_reshaped = X_train_ndarray.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Define the LSTM model\n",
    "LSTM_model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])), # LSTM layer with 64 neurons\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(X_train_reshaped, y_train.to_numpy(), epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:06:41.328789Z",
     "start_time": "2024-02-07T20:01:17.579331Z"
    }
   },
   "id": "7d7b0072a17698ef",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fitted_models = {\n",
    "    \"LSTM\": LSTM_model,\n",
    "    # \"ARIMA\": ARIMA(y_train, order=(5,1,0)),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"XGBoost\": xgb.XGBRegressor()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baf492e03f62058c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models evaluation and comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5203ad6ce68c34fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41e0b4ff0c40094e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary and conclusions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a34388c96a408b48"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0aa61f27666edc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "42c99198923450db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
